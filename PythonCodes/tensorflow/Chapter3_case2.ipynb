{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.8113182   1.4845988   0.06532937]\n",
      " [-2.4427042   0.0992484   0.5912243 ]]\n",
      "[[-0.8113182 ]\n",
      " [ 1.4845988 ]\n",
      " [ 0.06532937]]\n",
      "After 0 training steps, cross entropy on all data is 0.0674925\n",
      "After 1000 training steps, cross entropy on all data is 0.0163385\n",
      "After 2000 training steps, cross entropy on all data is 0.00907547\n",
      "After 3000 training steps, cross entropy on all data is 0.00714436\n",
      "After 4000 training steps, cross entropy on all data is 0.00578471\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from numpy.random import RandomState\n",
    "\n",
    "batch_size = 8 \n",
    "\n",
    "w1 = tf.Variable(tf.random_normal([2,3], stddev =1, seed = 1))\n",
    "w2 = tf.Variable(tf.random_normal([3,1], stddev =1, seed = 1 ))\n",
    "\n",
    "x = tf.placeholder(tf.float32, shape= (None,2), name = \"x-input\")\n",
    "y_ = tf.placeholder(tf.float32, shape= (None,1), name = \"y-input\")\n",
    "\n",
    "a = tf.matmul(x, w1)\n",
    "y = tf.matmul(a, w2)\n",
    "\n",
    "learning_rate = 0.001\n",
    "\n",
    "cross_entropy = -tf.reduce_mean(y_ * tf.log(tf.clip_by_value(y,1e-10, 1.0)))\n",
    "train_step = tf.train.AdamOptimizer(learning_rate).minimize(cross_entropy)\n",
    "\n",
    "rdm = RandomState(1)\n",
    "\n",
    "dataset_size = 128 \n",
    "X = rdm.rand(dataset_size,2)\n",
    "Y = [[int(x1+x2 < 1 )] for (x1, x2) in X]\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init_op = tf.initialize_all_variables()\n",
    "    sess.run(init_op)\n",
    "    print(sess.run(w1))\n",
    "    print(sess.run(w2))\n",
    "    w1 = [[-0.8, 1.48, 0.07],\n",
    "        [-2.4,0.099,0.59]]\n",
    "    w2 = [[-0.8], [1.48], -0.07]\n",
    "    STEPS = 5000\n",
    "    \n",
    "    for i in range(STEPS):\n",
    "        start = (i * batch_size) % dataset_size \n",
    "        end = min(start  + batch_size, dataset_size)\n",
    "        \n",
    "        sess.run(train_step, \n",
    "                 feed_dict = {x: X[start:end], y_: Y[start:end]})\n",
    "        if i % 1000 == 0:\n",
    "            total_cross_entropy = sess.run( cross_entropy, feed_dict = {x: X, y_: Y})\n",
    "            print(\"After %d training steps, cross entropy on all data is %g\" %(i,total_cross_entropy))\n",
    "            \n",
    "            \n",
    "    #print(sess.run(w1))\n",
    "    #print(sess.run(w2))\n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
